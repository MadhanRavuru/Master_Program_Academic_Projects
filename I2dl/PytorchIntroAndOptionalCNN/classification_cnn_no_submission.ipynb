{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image classification with CNNs\n",
    "================\n",
    "\n",
    "The goal of this exercise is to implement a specific CNN architecture with PyTorch and train it on the CIFAR-10 image classification dataset. We will start by introducing the dataset and then implement a `nn.Module` and a useful `Solver` class. Seperating the model from the actual training has proven itself as a sensible design decision. By the end of this exercise you should have succesfully trained your (possible) first CNN model and have a boilerplate `Solver` class which you can reuse for the next exercise and your future research projects.\n",
    "\n",
    "For an inspiration on how to implement a model or the solver class you can have a look at [these](https://github.com/pytorch/examples) PyTorch examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from exercise_code.classifiers.classification_cnn import ClassificationCNN\n",
    "from exercise_code.data_utils import get_CIFAR10_datasets, OverfitSampler, rel_error\n",
    "\n",
    "#torch.set_default_tensor_type('torch.FloatTensor')\n",
    "#set up default cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 Dataset\n",
    "=========\n",
    "\n",
    "Since the focus of this exercise should be neural network models and how to successfully train them, we provide you with preprocessed and prepared datasets. For an even easier management of the train, validation and test data pipelines we provide you with custom `torch.utils.data.Dataset` classes. Use the official [documentation](http://pytorch.org/docs/data.html) to make yourself familiar with the `Dataset` and `DataLoader` classes. Think about how you have to integrate them in your training loop and have a look at the data preprocessing steps in `dl4cv/data_utils.py`.\n",
    "\n",
    "The `num_workers` argument of the `DataLoader` class allows you to preprocess data with multiple threads.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>In this case we generated the <code>Dataset</code> classes after we applied all the preprocessing steps. Other datasets or random data augmentation might require an online preprocessing which can be integrated into the <code>Dataset</code> classes. See <code>torchvision.Transform</code> for examples.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the (preprocessed) CIFAR10 data. The preprocessing includes\n",
    "# channel swapping, normalization and train-val-test splitting.\n",
    "# Loading the datasets might take a while.\n",
    "\n",
    "train_data, val_data, test_data, mean_image = get_CIFAR10_datasets()\n",
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Val size: %i\" % len(val_data))\n",
    "print(\"Test size: %i\" % len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Examples\n",
    "------------------\n",
    "\n",
    "To make yourself familiar with the dataset we visualize some examples. We show a few examples from each class. Note that we have to revert (transposition and mean subtraction) some preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for cls_idx, cls in enumerate(classes):\n",
    "    cls_data = [datum for datum in test_data if datum[1] == cls_idx]\n",
    "    rnd_idxs = np.random.randint(0, len(cls_data), samples_per_class)\n",
    "    rnd_cls_data = [datum for i, datum in enumerate(cls_data) if i in rnd_idxs]\n",
    "    for i, cls_datum in enumerate(rnd_cls_data):\n",
    "        plt_idx = i * num_classes + cls_idx + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(cls_datum[0].numpy().transpose(1,2,0) + mean_image.transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture and Forward Pass \n",
    "\n",
    "After you understood the core concepts of PyTorch and have a rough idea on how to implement your own model, complete the initialization and forward methods of the `ClassificationCNN` in the `dl4cv/classifiers/classification_cnn.py` file. Note that we do not have to implement a backward pass since this is automatically handled by the `autograd` package.\n",
    "\n",
    "Use the cell below to check your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.classifiers.classification_cnn import ClassificationCNN\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.random.randn(2, 3, 5, 5).astype(np.float32)\n",
    "X_tensor = torch.from_numpy(X.copy())\n",
    "inputs = X_tensor.to(device)\n",
    "\n",
    "model = ClassificationCNN(input_dim=(3, 5, 5), num_classes=3)\n",
    "model.to(device)\n",
    "outputs = model.forward(inputs)\n",
    "correct_outputs = np.array([[0.0012621, -0.099135,  0.076110],\n",
    "                            [0.0013608, -0.099130,  0.076120]])\n",
    "\n",
    "# The difference should be very small. We get 1e-5\n",
    "print('Difference between the correct and your forward pass:')\n",
    "print(rel_error(correct_outputs, outputs.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation with the Solver\n",
    "We train and validate our previously generated model with a seperate `Solver` class defined in `dl4cv/solver.py`. Complete the `.train()` method and try to come up with an efficient iteration scheme as well as an informative training logger.\n",
    "\n",
    "Use the cells below to test your solver. A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>As seen below, the design of our <code>Solver</code> class is indepdenent of the particular model or data pipeline. This facilitates the reuse of the class and its modular structure allows the training of different models.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.classifiers.classification_cnn import ClassificationCNN\n",
    "from exercise_code.solver import Solver\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "num_train = 100\n",
    "OverfitSampler = SequentialSampler(range(num_train))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=50, shuffle=False, num_workers=4,\n",
    "                                           sampler=OverfitSampler)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=50, shuffle=False, num_workers=4)\n",
    "\n",
    "overfit_model = ClassificationCNN()\n",
    "overfit_model.to(device)\n",
    "overfit_solver = Solver(optim_args={\"lr\": 1e-2})\n",
    "overfit_solver.train(overfit_model, train_loader, val_loader, log_nth=1, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(overfit_solver.train_loss_history, 'o')\n",
    "plt.plot(overfit_solver.val_loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(overfit_solver.train_acc_history, '-o')\n",
    "plt.plot(overfit_solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "Now train your model with the full dataset. By training a `ThreeLayerCNN` model for one epoch, you should already achieve greater than 40% accuracy on the validation set. If your training is painfully slow check if you did not forget to call the `nn.Module.to()` method to transfer your model onto GPU.\n",
    "\n",
    "For the overfitting example we provided you with a set of hyperparamters (`hidden_dim`, `lr`, `weight_decay`, ...). You can start with the same parameter values but in order to maximize your accuracy you should try to train multiple models with different sets of hyperparamters. This process is called hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from exercise_code.classifiers.classification_cnn import ClassificationCNN\n",
    "from exercise_code.solver import Solver\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=50, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=50, shuffle=False, num_workers=4)\n",
    "\n",
    "############################################################################\n",
    "# TODO: Initialize a model and train it using your Solver class. Start     #\n",
    "# with the previously given set of hyperparameters.                        #\n",
    "############################################################################\n",
    "\n",
    "model = ClassificationCNN(hidden_dim=500)\n",
    "model.to(device)\n",
    "solver = Solver(optim_args={\"lr\": 1e-3, \"weight_decay\": 0.001})\n",
    "solver.train(model, train_loader, val_loader, log_nth=100, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filters\n",
    "You can visualize the first-layer convolutional filters from the trained network by running the following. If your kernel visualizations do not exhibit clear structures try optimizing the weight scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from exercise_code.vis_utils import visualize_grid\n",
    "\n",
    "# first (next) parameter should be convolutional\n",
    "conv_params = next(model.parameters()).cpu().data.numpy()\n",
    "grid = visualize_grid(conv_params.transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(6, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your Model\n",
    "Run your best model on the test set. See if you have a score above __60__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=50, shuffle=False, num_workers=4)\n",
    "\n",
    "scores = []\n",
    "model.eval()\n",
    "for inputs, targets in test_loader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    scores.extend(preds.cpu().numpy() == targets.cpu().numpy())\n",
    "\n",
    "print('Test set accuracy: %f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "When you are satisfied with your training, you can save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/classification_cnn.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Next Steps\n",
    "\n",
    "1. Hyperparameter optimization\n",
    "2. Data augmentation ([PyTorch tutorial](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html))\n",
    "3. Improve your network architecture\n",
    "    1. Increase network depth\n",
    "    2. Make network convolutional\n",
    "    2. Add additional layers such as [Batch normalization](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b#).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
